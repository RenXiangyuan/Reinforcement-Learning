{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Function Approximation for Q Learning\n",
    "\n",
    "Name: Xiangyuan Ren\n",
    "\n",
    "ID: A53249047"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cartpole\n",
    "\n",
    "A cartpole problem is shown below.\n",
    "![pendulum2.png](pendulum2.png)\n",
    "\n",
    "The equation for the cartpole problem is nonlinear in nature, but it has been shown through robust control theory that a linear version of the equation of the form $\\dot{x} = Ax+Bu$ can be solved by a linear controller. Let us assume that we are interested in minimizing cart stray from the center, and pendulum falling. It turns out that typical techniques - open loop control, PID control, root locus, etc. is not suitable for stabilizing both the cart position (keep near center) or the pole angle (keep vertical). The solution to this question is a linear quadratic controller, but we won't be using the solution at the moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Environment for Function Approximation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.01651899,  0.020169  ,  0.03030382,  0.0064973 ])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the CartPole game environment\n",
    "env = gym.make('CartPole-v0')\n",
    "env = env.unwrapped\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demonstrate your understanding of the simulation\n",
    "For OpenAI's CartPole-v0 environment,\n",
    "- describe the reward system\n",
    "- describe the each state variable (observation space)\n",
    "- describe the action space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 1. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\n",
    "     2. The state variable is [position of cart, velocity of cart, angle of pole, rotation rate of pole]\n",
    "     3. action space is a force of +1 or -1 to the cart\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a Deep Neural Network class that creates a dense network of a desired architecture\n",
    "In this problem we will create neural network that is our function that takes states to q-values: $q=f(x)$. While any function approximator could be used (i.e. Chebyshev functions, taylor series polynomials), neural networks offer a most general form of 1st-order smooth function (though comprising of trivial small activation functions means that complex functions require a significant amount of weights to identify). \n",
    "\n",
    "Create a class for a QNetwork that uses PyTorch to create a fully connected sequential neural network, of the following properties:\n",
    "- solver: Adam\n",
    "\n",
    "- input and hidden layer activation function: tanh\n",
    "\n",
    "- output activation function: linear\n",
    "\n",
    "- loss: mse\n",
    "\n",
    "- learning_rate: variable\n",
    "\n",
    "- decay_rate: variable\n",
    "\n",
    "- hidden_state sizes: variable\n",
    "\n",
    "- state and action sizes: variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size):\n",
    "        super(Net, self).__init__()\n",
    "        nn.Module.__init__(self)\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = F.relu(self.fc1(x))\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class QNetwork(object):#!!!!alpha decay\n",
    "    def __init__(self,learning_rate, state_size, action_size, hidden_size, alpha_decay):\n",
    "        self.qnet = Net(state_size, action_size, hidden_size)\n",
    "        self.optimizer = torch.optim.Adam(self.qnet.parameters(), lr=learning_rate)    # torch 的优化器\n",
    "        self.loss_func = nn.MSELoss()   # 误差公式\n",
    "    def optimize_model(self,batch):\n",
    "        x=Variable(torch.FloatTensor(batch[0]))\n",
    "        y=Variable(torch.FloatTensor(batch[1]))\n",
    "        a=Variable(torch.FloatTensor(batch[2]))\n",
    "        output = torch.sum(DQN.qnet(x)*a,1)\n",
    "        loss = self.loss_func(output, y)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write a Replay class that includes all the functionality of a replay buffer\n",
    "The replay buffer should kept to some maximum size (10000), allow adding of samples and returning of samples at random from the buffer. Each sample (or experience) is formed as (state, action, reward, next_state, done). The replay buffer should also be able to generate a minibatch. The generate_minibatch method should take in DQN, targetDQN, selected batch_size, and return the states present in the minibatch and the target Q values for those states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Replay():\n",
    "# Replay should also have an initialize method which creates a minimum buffer for \n",
    "# the initial episodes to generate minibatches.        \n",
    "    def __init__(self, max_size=10000):\n",
    "        self.max_size = max_size\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "    def initialize(self,init_length=1000, envir=env):\n",
    "        for iters in range(init_length):\n",
    "            done = False\n",
    "            envir.reset()\n",
    "            state= envir.state#.env.state\n",
    "            while not(done):\n",
    "                action=env.action_space.sample()\n",
    "                next_state, reward, done, info = envir.step(action)\n",
    "                if done:\n",
    "                    next_state=None\n",
    "                self.memory.append((state,action,reward,next_state,done))\n",
    "                if len(self.memory)>=self.max_size:\n",
    "                    return\n",
    "                state = next_state       \n",
    "    def push(self,state,action,reward,next_state,done):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        #if len(self.memory) < self.max_size:\n",
    "        #    self.memory.append(None)\n",
    "        self.memory[self.position] = (state,action,reward,next_state,done)\n",
    "        self.position = (self.position + 1) % self.max_size\n",
    "    def generate_minibatch (self, DQN, targetDQN, batch_size):\n",
    "        tmp = random.sample(self.memory, batch_size)\n",
    "        states = np.zeros((batch_size,4))\n",
    "        qvalues= np.zeros(batch_size)\n",
    "        actions= np.zeros((batch_size,2))\n",
    "        j = 0\n",
    "        for i in tmp:\n",
    "            \n",
    "            s1= i[0]; s2=i[3]; r = i[2]; a=i[1]\n",
    "            #|print(s1,s2,r,a)\n",
    "            if type(s2)==type(None):\n",
    "                qvalue = r\n",
    "            else:\n",
    "                r1,r2 = targetDQN.qnet.forward(Variable(torch.FloatTensor(s2))).data\n",
    "                #print([r1,r2])\n",
    "                qvalue= r + gamma * np.max([r1,r2]) #- DQN.qnet.forward(Variable(torch.FloatTensor(s1))).data[i[1]]\n",
    "            states[j,:]=s1\n",
    "            qvalues[j]=qvalue; actions[j,a]=1\n",
    "            j+=1\n",
    "        return states,qvalues,actions\n",
    "    def __len__(self):\n",
    "        return len(self.memory)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that creates a minibatch from a buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Function Approximation\n",
    "Initialize DQN networks and Replay objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DQN\n",
    "\n",
    "# Play around with your learning rate, alpha decay and hidden layer units \n",
    "# Two layers with a small number of units should be enough\n",
    "learning_rate, state_size, action_size, hidden_size, alpha_decay = 1e-3, 4, 2, 10,0.99\n",
    "DQN = QNetwork(learning_rate, state_size, action_size, hidden_size, alpha_decay)\n",
    "targetDQN = QNetwork(learning_rate, state_size, action_size, hidden_size, alpha_decay)\n",
    "targetDQN.qnet.load_state_dict(DQN.qnet.state_dict())\n",
    "# set targetDQN weights to DQN weights\n",
    "# for ex. targetDQN.model.weights = DQN.model.weights (syntax given here is for representation purpose only)\n",
    "\n",
    "## Initialize Replay Buffer\n",
    "###################################\n",
    "## Populate the initial experience buffer\n",
    "###################################\n",
    "\n",
    "replay = Replay(max_size=10000)\n",
    "replay.initialize(init_length=1000, envir=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a function that solves the above environment using a deep Q network that uses a minibatch strategy.\n",
    "Use the following parameters (these had to be derived empirically - there is generally no trusted way of choosing the right parameter values - i.e. gamma, number of episodes, decay rate, min_epsilon). \n",
    "\n",
    "Generate a graph of the average return per episode every 100 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state,epsilon):\n",
    "    sample = random.random()\n",
    "    eps_threshold = epsilon\n",
    "    if sample > eps_threshold:\n",
    "        tmp=Variable(torch.FloatTensor(state), volatile=True).type(torch.FloatTensor)\n",
    "        return np.argmax(DQN.qnet(tmp).data)\n",
    "    else:\n",
    "        return np.random.choice([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25   20.0\n",
      "50   13.0\n",
      "75   38.0\n",
      "100   16.0\n",
      "125   49.0\n",
      "150   137.0\n",
      "175   94.0\n",
      "200   20.0\n",
      "225   129.0\n",
      "250   44.0\n",
      "275   140.0\n",
      "300   220.0\n",
      "325   141.0\n",
      "350   220.0\n",
      "375   123.0\n",
      "400   188.0\n",
      "425   119.0\n",
      "450   27.0\n",
      "475   220.0\n",
      "500   220.0\n",
      "525   220.0\n",
      "550   116.0\n",
      "575   34.0\n",
      "600   220.0\n",
      "625   71.0\n",
      "650   220.0\n",
      "675   220.0\n",
      "700   19.0\n",
      "725   220.0\n",
      "750   10.0\n",
      "775   161.0\n",
      "800   220.0\n",
      "825   140.0\n",
      "850   149.0\n",
      "875   119.0\n",
      "900   109.0\n",
      "925   220.0\n",
      "950   220.0\n",
      "975   220.0\n",
      "1000   220.0\n",
      "1025   220.0\n",
      "1050   46.0\n",
      "1075   220.0\n",
      "1100   220.0\n",
      "1125   220.0\n",
      "1150   12.0\n",
      "1175   220.0\n",
      "1200   170.0\n",
      "1225   220.0\n",
      "1250   220.0\n",
      "1275   102.0\n",
      "1300   220.0\n",
      "1325   146.0\n",
      "1350   148.0\n",
      "1375   220.0\n",
      "1400   220.0\n",
      "1425   10.0\n",
      "1450   219.0\n",
      "1475   220.0\n",
      "1500   40.0\n",
      "1525   38.0\n",
      "1550   220.0\n",
      "1575   220.0\n",
      "1600   150.0\n",
      "1625   220.0\n",
      "1650   220.0\n",
      "1675   12.0\n",
      "1700   220.0\n",
      "1725   220.0\n",
      "1750   220.0\n",
      "1775   220.0\n",
      "1800   220.0\n",
      "1825   220.0\n",
      "1850   220.0\n",
      "1875   220.0\n",
      "1900   220.0\n",
      "1925   220.0\n",
      "1950   220.0\n",
      "1975   220.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Runtime parameters\n",
    "\n",
    "gamma = 0.99                  # future reward discount\n",
    "# Exploration parameters\n",
    "min_epsilon = 0.01             # minimum exploration probability\n",
    "\n",
    "hidden_size=20\n",
    "TARGET_UPDATE = 10\n",
    "batch_size = 32\n",
    "num_episodes = 2000\n",
    "decay_rate = 5/num_episodes    # exponential decay rate for exploration prob\n",
    "max_steps = 220                # cut off simulation after this many steps\n",
    "state_size, action_size=4,2\n",
    "\n",
    "returns = np.zeros(num_episodes)\n",
    "for ep in range(1, num_episodes):\n",
    "    #print(\"ep:\",ep)\n",
    "    epsilon = min_epsilon + (1.0 - min_epsilon)*np.exp(-decay_rate*ep)\n",
    "    \n",
    "    env.reset()\n",
    "    state = env.state\n",
    "    total_reward=0\n",
    "    #print(\"\\tt starts\")\n",
    "    if ep>400 and returns[-1]>180:\n",
    "        mode=True\n",
    "    else:\n",
    "        mode=False\n",
    "    for t in range(max_steps):# --> start episode \n",
    "        #print(\"\\tt:\",t)\n",
    "        action = select_action(state,epsilon)# explore/exploit and get action using DQN\n",
    "        next_state, reward, done, _ = env.step(action)# perform action and record new_state, action, reward\n",
    "        if done:\n",
    "            next_state = None\n",
    "        replay.push(state,action,reward,next_state,done)# populate Replay experience buffer\n",
    "        state = next_state\n",
    "        total_reward+=reward\n",
    "        if not(mode):\n",
    "            batch = replay.generate_minibatch(DQN, targetDQN, batch_size)\n",
    "            DQN.optimize_model(batch)\n",
    "            if (t%TARGET_UPDATE==0):\n",
    "                targetDQN.qnet.load_state_dict(DQN.qnet.state_dict())\n",
    "        if done:\n",
    "            break\n",
    "    # <-- end episode\n",
    "    #print(\"\\tt ends\")\n",
    "    if ep%25==0:\n",
    "        print(ep,' ',total_reward)\n",
    "    returns[ep] = total_reward\n",
    "    \n",
    "    # Replay\n",
    "    if mode:\n",
    "        batch = replay.generate_minibatch(DQN, targetDQN, batch_size)\n",
    "        DQN.optimize_model(batch)   \n",
    "        if ep % TARGET_UPDATE == 0:\n",
    "            targetDQN.qnet.load_state_dict(DQN.qnet.state_dict())\n",
    "    \n",
    "    \n",
    "    # set targetDQN weights to DQN weights\n",
    "    # update DQN (run one epoch of training per episode with generated minibatch of states and qvalues)\n",
    "    \n",
    "#     total_reward=0\n",
    "#     state = env.reset()\n",
    "#     for t in range(max_steps):# --> start episode \n",
    "#         #print(\"\\tt:\",t)\n",
    "#         action = select_action(state,0)# explore/exploit and get action using DQN\n",
    "#         next_state, reward, done, _ = env.step(action)# perform action and record new_state, action, reward\n",
    "#         #replay.push(state,action,reward,next_state,done)# populate Replay experience buffer\n",
    "#         state = next_state\n",
    "#         total_reward+=reward\n",
    "#         #batch = replay.generate_minibatch(DQN, targetDQN, batch_size)\n",
    "#         #targetDQN.qnet.load_state_dict(DQN.qnet.state_dict())\n",
    "#         if done:\n",
    "#             break  \n",
    "#     returns[ep]=total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114f8d908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot average returns\n",
    "returns_over_100_episodes = []\n",
    "x = []\n",
    "for i in range(0,int(num_episodes/100)):\n",
    "    returns_over_100_episodes.append(sum(returns[100*i:100*(i+1)-1])/100)\n",
    "    x.append((i+1)*100)\n",
    "plt.plot(x,returns_over_100_episodes,'.-r')\n",
    "plt.ylabel('Average Returns per Episode')\n",
    "plt.xlabel('Num of Episodes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEMO FINAL NETWORK\n",
    "env.reset()\n",
    "# Take one random step to get the pole and cart moving\n",
    "state, reward, done, _ = env.step(env.action_space.sample())\n",
    "state = np.reshape(state, [1, state.size])\n",
    "total_reward = 0\n",
    "for i in range(0, max_steps):\n",
    "    env.render()\n",
    "    \n",
    "    # Get action from Q-network\n",
    "    tmp=Variable(torch.FloatTensor(state), volatile=True).type(torch.FloatTensor)\n",
    "    Qs =DQN.qnet(tmp).data\n",
    "    # Qs = output of DQN.model when state is passed in\n",
    "    action = np.argmax(Qs)\n",
    "    \n",
    "    # Take action, get new state and reward\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "    else:\n",
    "        state = np.reshape(next_state, [1, state.size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
